{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAJaRSseDCrU"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 2, Module 1*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAJaRSseDCrU",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Neural Networks (Prepare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAJaRSseDCrU"
   },
   "source": [
    "## Learning Objectives\n",
    "* <a href=\"#p1\">Part 1</a>: Describe the foundational components of a neural network\n",
    "* <a href=\"#p2\">Part 2</a>: Implement a Perceptron from scratch in Python\n",
    "\n",
    "Neural Networks are a whole new area of study and application that can be intimidating, but which represents some of the most powerful tools and techniques that we possess in machine learning today. In spite of the hype surrounding these topics I hope that you will come to see them as just another tool in your tool bag with their own strengths and weaknesses. They are useful, but they are not a silver bullet, and they are not always preferable to other -perhaps more simple- machine learning methods. \n",
    "\n",
    "The goal of this week is to familiarize you with the fundamental theory, terminology and libraries that will enable you to approach different neural network architectures (called topologies) in a sophisticated manner. This week will not be a run-through of the history of Neural Networks and each of the individual advancements leading up to current technologies -we don't have time for that. We will spend some time on some older methods, but only to the degree that they will help introduce us to relevant terminology and understand more complex versions of these technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCyHLvj4HsrQ",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Foundational Components of Neural Networks (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCyHLvj4HsrQ",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Overview\n",
    "\n",
    "Neural Networks aren't exactly a new technology, but recent breakthroughs have revitalized the area. The idea using math to represent the brain has been around since 1943, and the \"Perceptron\" - one of the basic building blocks of the technology- was invented in 1958. \n",
    "\n",
    "Artificial Neural Networks are a computational model that was inspired by how neural networks in the brain process information. In the brain electrochemical signals flow from earlier neurons through the dendrites of the cell toward the cell body. If the received signals surpass a certain threshold with a given timing then the neuron fires sending a large spike of energy down the axon and through the axon terminals to other neurons down the line. \n",
    "\n",
    "![Wikipedia Neuron Diagram](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Neuron.svg/500px-Neuron.svg.png)\n",
    "\n",
    "In Artificial Neural Networks the neurons or \"nodes\" are similar in that they receive inputs and pass on their signal to the next layer of nodes if a certain threshold is reached, but that's about where the similarities end. Remember that ANNs are not brains. Don't fall into the common trap of assuming that if an Artificial Neural Network has as many nodes as the human brain that it will be just as powerful or just as capable. The goal with ANNs is not to create a realistic model of the brain but to craft robust algorithms and data structures that can model the complex relationships found in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WUaY3-inOrea",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Say you want to buy a house, you're quoted $400,000 for a 2000 square foot house. Is that good? Is that bad? How can we know? Well, we can gather up some more prices of other homes in the area and their square footages to compare.\n",
    "\n",
    "| Square Footage | Price  |\n",
    "|----------------|--------|\n",
    "| 2104           | 399900 |\n",
    "| 1600           | 329900 |\n",
    "| 2400           | 369000 |\n",
    "\n",
    "Ok, so now we have a little bit more data to compare to, how can we compare the price that we're getting to the prices of these houses? We can't compare price directly because all of these homes are different sizes, so we decide to calculate an average price per square foot for these three homes and we'll compare that to the average square footage price for the house that we have been looking at.\n",
    "\n",
    "What does our neural network predict should be the price of the house that we are looking at?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "28tLn2-XSe7y"
   },
   "outputs": [],
   "source": [
    "price = 2000 * 180\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LhhsQFuyVuR0",
    "toc-hr-collapsed": true
   },
   "source": [
    "### Node Maps\n",
    "\n",
    "We've created our first *extremely* basic neural network, it takes an input, modifies it by a weight (180) and reports an output. Please, instead of thinking about neural networks as if they were brains think about them as a function or a \"mapping\" from inputs to outputs just like we have established many times before. \n",
    "\n",
    "![A Mapping](http://jalammar.github.io/images/NNs_formula_no_bias.png)\n",
    "\n",
    "What we have here above is what's known as a \"Node Map\" it's a visual diagram of the architecture or \"topology\" of our neural network. It's kind of like a flow chart in that it shows the path from inputs to outputs. They are usually color coded and help us understand at a very high level, some of the differences in architecture between kinds of neural networks. Just like with all the areas of machine learning that we have studied before there is a \"[zoo](http://www.asimovinstitute.org/neural-network-zoo/)\" of neural network architectures:\n",
    "\n",
    "![Neural Network Zoo](http://www.asimovinstitute.org/wp-content/uploads/2016/09/neuralnetworks.png)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6fYbPmJ-ZDWt"
   },
   "source": [
    "### Types of Layers:\n",
    "\n",
    "There are three main types of neuron layers in a typical NN topology:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6fYbPmJ-ZDWt"
   },
   "source": [
    "#### Input or Visible Layers\n",
    "\n",
    "The Input Layer is what receives input from our dataset. Sometimes it is called the visible layer because it's the only part that is exposed to our data and that our data interacts with directly. Typically node maps are drawn with one input node for each of the different inputs/features/columns of our dataset that will be passed to the network.\n",
    "\n",
    "#### Hidden Layers\n",
    "\n",
    "Layers after the input layer are called Hidden Layers. This is because they cannot be accessed except through the input layer. They're inside of the network and they perform their functions, but we don't directly interact with them. The simplest possible network is to have a single neuron in the hidden layer that just outputs the value. \"Deep Learning\" apart from being a big buzzword simply means that we are using a Neural Network that has multiple hidden layers. \"Deep Learning\" is a big part of the renewed hype around ANNs because it allows networks that are structured in specific ways to accomplish tasks that were previously out of reach (image recognition for example).  \n",
    "\n",
    "#### Output Layers\n",
    "\n",
    "The final layer is called the Output Layer. The purpose of the output layer is to output a vector of values that is in a format that is suitable for the type of problem that we're trying to address. Typically the output value is modified by an \"activation function\" to transform it into a format that makes sense for our context, here's a couple of examples:\n",
    "\n",
    "- NNs applied to a regression problem might have a single output node with no activation function because what we want is an unbounded continuous value.\n",
    "\n",
    "- NNS applied to a binary classification problem might use a sigmoid function as its activation function in order to squishify values down to represent a probability. Outputs in this case would represent the probability of predicting the primary class of interest. We can turn this into a class-specific prediction by rounding the outputted sigmoid probability up to 1 or down to 0. \n",
    "\n",
    "- NNS applied to multiclass classification problems might have multiple output nodes in the output layer, one for each class that we're trying to predict. This output layer would probably employ what's called a \"softmax function\" for accomplishing this. Don't worry about how that activation function works just yet, we'll get to it soon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbOJUGsddcJA"
   },
   "source": [
    "### Back to our House Problem\n",
    "\n",
    " Is a neural network that predicts home prices based on the average square footage of homes in the are a good one? How can we evaluate the quality of our predictions?\n",
    " \n",
    "We evaluate the quality of our predictions just like we would any machine learning algorithm. We look at what the algorithm predicted and compare it to the actual price to see how far off we are. We know that our predictions are getting better if our predictions get closer to the actual prices in our training data / testing data. \n",
    "\n",
    "So how is our Neural Network doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "zxnRa-bzB-m2",
    "outputId": "ed67e674-3755-4fe1-c22c-befc91380c41"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [2104, 1600, 2400]\n",
    "y = [399.9, 329.9, 369]\n",
    "predictions = [379, 288, 432]\n",
    "difference = [21, 42, -63]\n",
    "difference_squared = [449, 1756, 3969]\n",
    "\n",
    "df = pd.DataFrame({'sqft': x, \"price_in_thous\": y, \"predictions\": predictions, \n",
    "                   \"difference\": difference, \"diff_squared\": difference_squared})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "JgKs9py9fBuM",
    "outputId": "2d16ce33-a8ad-4052-e4b7-3d4717305389"
   },
   "outputs": [],
   "source": [
    "plt.scatter(df.sqft, df.price_in_thous)\n",
    "plt.xlabel(\"Square Feet\")\n",
    "plt.ylabel(\"Price in Thousands of Dollars\")\n",
    "plt.title(\"Home Price Estimates\")\n",
    "\n",
    "# Plot the line\n",
    "price_per_sqft = .180\n",
    "\n",
    "y_hat = [price_per_sqft*x for x in df.sqft]\n",
    "plt.plot(df.sqft, y_hat, color=\"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_422Q5XgTo5"
   },
   "source": [
    "### Look Familiar?\n",
    "\n",
    "Well it should because we've just plotted a regression line hypothesis based on the average square foot price of the three homes that we looked at. Our average error for this line is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CY6xiQ-BgxLR",
    "outputId": "68de5027-fa5a-4555-e909-f1f5fda536a3"
   },
   "outputs": [],
   "source": [
    "print(\"Mean Squared Error:\", df.diff_squared.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mesjb_tfg2zQ"
   },
   "source": [
    "Now, we didn't fit this line using any regression techniques so it's likely that it's **not** the line of best fit. But we could try out other lines and see if the error gets bigger or smaller. Well it just so happens that in this two dimensional world that if our line is stuck to the origin on one end then we don't have all of the freedom that we want to control our predictions. We need some way to slide it up and down the y axis so that we can make any line that we want in this two-dimensional world. This y-intercept is a constant value, or in other words it's always weighted by 1. We're going to call this y-intercept our \"bias\" term. We'll define this a little bit better in a minute. \n",
    "\n",
    "![New Network with Bias](http://jalammar.github.io/images/NNs_bias.png)\n",
    "\n",
    "Now I don't love this diagram because it shows our \"bias\" term as if it was a new horizontal layer, in reality the bias term interacts with a single layer to affect all layers after the hidden layer that it is associated with. This diagram will give you a better idea: \n",
    "\n",
    "![Bias Terms](https://i.stack.imgur.com/6S6Bz.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "y_7t3vOBfYDz",
    "outputId": "a0ad47ab-f923-47d0-9268-85154768427f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "weight = .100\n",
    "bias = 160\n",
    "\n",
    "X = np.array([2104, 1600, 2400])\n",
    "y = np.array([399.9, 329.9, 369])\n",
    "predictions = [weight*x + bias for x in X]\n",
    "print(\"Predictions:\", predictions)\n",
    "difference = y-predictions\n",
    "print(\"Difference:\", difference)\n",
    "difference_squared = difference**2\n",
    "print(\"Difference Squared:\", difference_squared)\n",
    "MSE = difference_squared.sum()/len(difference_squared)\n",
    "print(\"Mean Squared Error\", MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "gdCy8l7Sj6nW",
    "outputId": "2ae1aa54-ff2f-4a15-9eb7-709d36ed501a"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.xlabel(\"Square Feet\")\n",
    "plt.ylabel(\"Price in Thousands of Dollars\")\n",
    "plt.title(\"Home Price Estimates\")\n",
    "\n",
    "# Plot the line\n",
    "price_per_sqft = .180\n",
    "\n",
    "y_hat = [weight*x + bias for x in X]\n",
    "plt.plot(X, y_hat, color=\"green\")\n",
    "\n",
    "y_hat = [price_per_sqft*x for x in df.sqft]\n",
    "plt.plot(df.sqft, y_hat, color=\"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ra7k-EJBj5_o"
   },
   "source": [
    "The bias value in coordination with our weight gives our network the full range of motion that it needs to find the best way to explain the patterns in the data. \n",
    "\n",
    "Well, what if instead of only looking at square footage we collected a second data point like number of bathrooms there is in each house. Lets also get a few more data points while we're at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "UTZYAnU6myqP",
    "outputId": "e865a9a6-fea9-4139-c2b3-2b6fdd3b35a9"
   },
   "outputs": [],
   "source": [
    "sqft = [2104, 1600, 2400, 1416, 3000, 1985, 1534, 1427, 1380, 1419]\n",
    "bathrooms = [3, 3, 3, 2, 4, 4, 3, 3, 3, 3]\n",
    "price = [399900, 329900, 369000, 232000, 539000, 299900, 314900, 198999,\n",
    "        212000, 242500]\n",
    "\n",
    "df = pd.DataFrame({'sqft': sqft, 'bathrooms': bathrooms, 'price': price})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TIEht5FKoFfg"
   },
   "source": [
    "### Two Input Nodes\n",
    "\n",
    "Now that we have two inputs or two x variables how might we modify the architecture of our network to accept these inputs?\n",
    "\n",
    "![Two input Nodes](http://jalammar.github.io/images/NNs_2_variables.png)\n",
    "\n",
    "You'll notice that now we need a weight for each one of our input nodes, or for each feature of our dataset. If we were to write out the calculation that is happening as our data flows through this network it might look something like this:\n",
    "\n",
    "![Equation](http://jalammar.github.io/images/NNs_formula_two_variables.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TIEht5FKoFfg"
   },
   "source": [
    "### I hope this looks extremely familiar.\n",
    "\n",
    "Now we have a new network that takes in two inputs. But now instead of guessing at slope and coefficient values and evaluating them. How can we explore more combinations of weights and bias until we find the values that minimize our Mean-Squared Error?\n",
    "\n",
    "[Lets Try It!](http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iIpNgBlupRpn"
   },
   "source": [
    "### ANNs are not Linear Regression\n",
    "\n",
    "It just so happens that the one that we have defined in this way acts the same way. We've defined a neural network that does the same thing as Linear Regression for a couple of different reasons\n",
    "\n",
    "1) To familiarize you with node maps (even if I don't love the ones in the article).\n",
    "\n",
    "2) To introduce the concepts of \"weights\" and \"biases\" in a context that you're already familiar with.\n",
    "\n",
    "3) Each layer in a feed-forward neural network affects the next layer by a weighted sum of inputs plus a bias term. \n",
    "\n",
    "4) To make it plain to you that we can search out the optimal weights and biases of a neural network through gradient descent if we have a loss function that evaluates the quality of our predictions compared to the y values in our training data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OZC5EFQeu4Me",
    "toc-hr-collapsed": true
   },
   "source": [
    "### Why are Neural Networks so Powerful?\n",
    "\n",
    "So if we can use a very simple neural network to represent a linear regression problem, what do neural networks with lots of additional features/layers represent? Why can't we just use linear regression for everything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OZC5EFQeu4Me"
   },
   "source": [
    "#### Nonlinearities\n",
    "\n",
    "Linear regression is built to fit more or less linear models in n-dimensional space. While it is true that we can fit non-linear features using linear regression as we include polynomial features, the reality is that in order to fit really curvy nonlinear patterns in data in really complex high dimensional features spaces, the number of polynomial terms that we would have to include in a linear or logistic regression model faces a problem of combinatorial explosion in terms of the number of features that would be required. \n",
    "\n",
    "Well what kinds of data exhibit these really strong nonlinearities? Well, how about images for example:\n",
    "\n",
    "![Cars](https://ak7.picdn.net/shutterstock/videos/4939097/thumb/1.jpg)\n",
    "\n",
    "Think about all of the different forms a car could take in an image yet our human brains recognize them as cars flawlessly. Think about all of the complexity that we would have to take into account. How could you even begin to feature engineer a dataset of pixel values in order to give a regression algorithm something that it could cue on as being a car or not a car?\n",
    "\n",
    "[Andrew Ng can probably explain this better than I can](https://www.youtube.com/watch?v=1ZhtwInuOD0)\n",
    "\n",
    "The interactions between layers of neurons in neural networks in a way accounts for that combinatorial explosion within the structure of the algorithm as needed instead of us having to provide it beforehand.\n",
    "\n",
    "![Neural Network](https://cdn-images-1.medium.com/max/1200/1*DW0Ccmj1hZ0OvSXi7Kz5MQ.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "In the corresponding module project, you will be asked to summarize the different components of a neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dGb0yyBtBCBD",
    "toc-hr-collapsed": false
   },
   "source": [
    "# The Perceptron (Learn)\n",
    "<a id=\"p2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dGb0yyBtBCBD"
   },
   "source": [
    "## Overview\n",
    "The first and simplest kind of neural network that we could talk about is the perceptron. A perceptron is just a single node or neuron of a neural network with nothing else. It can take any number of inputs and spit out an output. What a neuron does is it takes each of the input values, multiplies each of them by a weight, sums all of these products up, and then passes the sum through what is called an \"activation function\" the result of which is the final value.\n",
    "\n",
    "I really like figure 2.1 found in this [pdf](http://www.uta.fi/sis/tie/neuro/index/Neurocomputing2.pdf) even though it doesn't have bias term represented there.\n",
    "\n",
    "If we were to write what is happening in some verbose mathematical notation, it might look something like this:\n",
    "\n",
    "\\begin{align}\n",
    " y = sigmoid(\\sum(weight_{1}input_{1} + weight_{2}input_{2} + weight_{3}input_{3}) + bias)\n",
    "\\end{align}\n",
    "\n",
    "Understanding what happens with a single neuron is important because this is the same pattern that will take place for all of our networks. \n",
    "\n",
    "When imagining a neural network I like to think about the arrows as representing the weights, like a wire that has a certain amount of resistance and only lets a certain amount of current through. And I like to think about the node itself as containing the prescribed activation function that neuron will use to decide how much signal to pass onto the next layer.\n",
    "\n",
    "### Activation Functions (transfer functions)\n",
    "\n",
    "In Neural Networks, each node has an activation function. Each node in a given layer typically has the same activation function. These activation functions are the biggest piece of neural networks that have been inspired by actual biology. The activation function decides whether a cell \"fires\" or not. Sometimes it is said that the cell is \"activated\" or not. In Artificial Neural Networks activation functions decide how much signal to pass onto the next layer. This is why they are sometimes referred to as transfer functions because they determine how much signal is transferred to the next layer.\n",
    "\n",
    "##### Common Activation Functions:\n",
    "\n",
    "![Activation Functions](http://www.snee.com/bobdc.blog/img/activationfunctions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-9x3x5XgtD3i",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along\n",
    "### Implementing a Perceptron from scratch in Python\n",
    "\n",
    "Disclaimer: This is not the same Perceptron first created by Frank Rosenblatt which used the \"Step Function\" and the \"Perceptron Learning Rule\" instead of the sigmoid activation function and gradient descent respectively. Our Perceptron is more similiar to a modern Neural Network. Just a very simple one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A47bcPUYYf8S"
   },
   "source": [
    "### Establish training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9Sj_AVzReca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [1, 1, 1],\n",
       "       [1, 0, 1],\n",
       "       [0, 1, 1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(812)\n",
    "\n",
    "inputs = np.array([\n",
    "    [0,0,1],\n",
    "    [1,1,1],\n",
    "    [1,0,1],\n",
    "    [0,1,1]\n",
    "])\n",
    "\n",
    "correct_outputs = [[0], [1], [1], [0]]\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJYHTTThYlcj"
   },
   "source": [
    "### Sigmoid activation function and its derivative for updating weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-23) == np.exp(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWyVzV-oUTC8",
    "toc-hr-collapsed": true
   },
   "source": [
    "### Updating weights with derivative of sigmoid function:\n",
    "\n",
    "![Sigmoid Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iHpgMkpQX9HK"
   },
   "source": [
    "### Initialize random weights for our three inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 2 * np.random.random((3,1)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0099616 ],\n",
       "       [ 0.21185521],\n",
       "       [-0.08502562]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNhy_Qk2YBJO"
   },
   "source": [
    "### Calculate weighted sum of inputs and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ex5b896fWrMQ"
   },
   "outputs": [],
   "source": [
    "weighted_sum = np.dot(inputs, weights)\n",
    "weighted_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4z3LqLFaWMy"
   },
   "source": [
    "### Output the activated value for the end of 1 training epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "llmNHRH5W4BA"
   },
   "outputs": [],
   "source": [
    "activated_output = sigmoid(weighted_sum)\n",
    "activated_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_P2i3KEaiVC"
   },
   "source": [
    "### take difference of output and true values to calculate error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6mg175C6XaYO"
   },
   "outputs": [],
   "source": [
    "error = correct_outputs - activated_output\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent/backprop - magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjustments = error * sigmoid_derivative(weighted_sum)\n",
    "adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJH_wpb-XmKJ"
   },
   "outputs": [],
   "source": [
    "weights += np.dot(inputs.T, adjustments)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tc928NEda0UE"
   },
   "source": [
    "### Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5PGZR_caa18g"
   },
   "outputs": [],
   "source": [
    "# Steps we've already done: \n",
    "# 1. Randomly Initialized Weights already. Those are in memory as `weights`\n",
    "# 2. We've already got input data & correct_outputs\n",
    "\n",
    "\n",
    "# Update our weights 10,000 times - (fingers crossed that this process reduces error)\n",
    "for iteration in range(10000):\n",
    "    \n",
    "    # Weighted sum of inputs / weights\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    # Activate!\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    \n",
    "    # Cac error\n",
    "    error = correct_outputs - activated_output\n",
    "    \n",
    "    adjustments = error * sigmoid_derivative(weighted_sum)\n",
    "    \n",
    "    # Update the Weights\n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "    \n",
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sCeW6EVAu-jt"
   },
   "source": [
    "### Perceptrons can classify linearly separable classes\n",
    "\n",
    "This is a demo of code prepared by K Hong in a blog post available [here](https://www.bogotobogo.com/python/scikit-learn/Perceptron_Model_with_Iris_DataSet.php). Hong's implementation is based on a implementation in __Python Machine Learning__\n",
    "by Sebastian Raschka. \n",
    "\n",
    "This demo is here for two reasons: \n",
    "1. Show you a full class implementation of a perceptron (something you'll be working on in the assignment)\n",
    "2. Show graphically a perceptron ability to linearly separate classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "EYbebbPwu91r",
    "outputId": "b5e143ab-4148-41c6-bee4-20219aa9633d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "\n",
    "# Grab 50 versicolor and 50 virginica\n",
    "df.iloc[145:150, 0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "CgwUISn9u8xo",
    "outputId": "7b73e9e7-fc69-47bd-f16d-9c4bbdf4c5bc"
   },
   "outputs": [],
   "source": [
    "y = df.iloc[0:100, 4].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "HpLmUPCau8Nm",
    "outputId": "e930dd42-04ed-462c-a247-2ca71f686b13"
   },
   "outputs": [],
   "source": [
    "y = np.where(y == 'Iris-setosa', -1, 1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1751
    },
    "colab_type": "code",
    "id": "3ngdKVNhu7h1",
    "outputId": "6373c6d3-fd87-4d7e-c5dc-1d5bce64ae5d"
   },
   "outputs": [],
   "source": [
    "X = df.iloc[0:100, [0, 2]].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "qMwijtrju62D",
    "outputId": "b36afd2f-71e9-4bbf-f0a9-472f7ee4cc2a"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label='setosa')\n",
    "plt.scatter(X[50:100, 0], X[50:100, 1], color='blue', marker='x', label='versicolor')\n",
    "plt.xlabel('petal length')\n",
    "plt.ylabel('sepal length')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Ax_1Ewsu59o"
   },
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, rate = 0.01, niter = 10):\n",
    "        self.rate = rate\n",
    "        self.niter = niter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data\n",
    "        X : Training vectors, X.shape : [#samples, #features]\n",
    "        y : Target values, y.shape : [#samples]\n",
    "        \"\"\"\n",
    "\n",
    "        # weights\n",
    "        self.weight = np.zeros(1 + X.shape[1])\n",
    "\n",
    "        # Number of misclassifications\n",
    "        self.errors = []  # Number of misclassifications\n",
    "\n",
    "        for i in range(self.niter):\n",
    "          err = 0\n",
    "          for xi, target in zip(X, y):\n",
    "            delta_w = self.rate * (target - self.predict(xi))\n",
    "            self.weight[1:] += delta_w * xi\n",
    "            self.weight[0] += delta_w\n",
    "            err += int(delta_w != 0.0)\n",
    "          self.errors.append(err)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.weight[1:]) + self.weight[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        \"\"\" Default Step Function\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "ftFsGyvpu5iC",
    "outputId": "b9ce6e1e-f84f-498b-9dd9-04eb8b328f5f"
   },
   "outputs": [],
   "source": [
    "pn = Perceptron(0.1, 10)\n",
    "pn.fit(X, y)\n",
    "plt.plot(range(1, len(pn.errors) + 1), pn.errors, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of misclassifications')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4VAnfC7fu5P7"
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "  # setup marker generator and color map\n",
    "  markers = ('s', 'x', 'o', '^', 'v')\n",
    "  colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "  cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "  # plot the decision surface\n",
    "  x1_min, x1_max = X[:,  0].min() - 1, X[:, 0].max() + 1\n",
    "  x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "  xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "  np.arange(x2_min, x2_max, resolution))\n",
    "  Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "  Z = Z.reshape(xx1.shape)\n",
    "  plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "  plt.xlim(xx1.min(), xx1.max())\n",
    "  plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "  # plot class samples\n",
    "  for idx, cl in enumerate(np.unique(y)):\n",
    "    plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "    alpha=0.8, c=cmap(idx),\n",
    "    marker=markers[idx], label=cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "colab_type": "code",
    "id": "ntqksaQjr7Cm",
    "outputId": "4967f3f7-3c05-4606-f757-6f5aac802696"
   },
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, classifier=pn)\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will apply the perceptron to a dataset during today's assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ucUP72uiyQ_0",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Sources\n",
    "Knowledge doesn't come out of a vacuum. Neither does our code. We build off the work of other incredibly intelligent and harding work people. The academic and impementation sections are our way of saying **Thank You** to them. The external review material is stuff we've watched or read in the past we think could also help you. \n",
    "\n",
    "## Academic References\n",
    "(i.e. Theory and research we referenced in preparing this content)\n",
    "- McCulloch, W.S. & Pitts, W. Bulletin of Mathematical Biophysics (1943) 5: 115. https://doi.org/10.1007/BF02478259\n",
    "- Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6), 386–408. https://doi.org/10.1037/h0042519\n",
    "- Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning.\n",
    "\n",
    "## Implementation References\n",
    "(i.e. Stuff we used / referenced to make the code in this notebook)\n",
    "- [NN-SVG](http://alexlenail.me/NN-SVG/index.html) by Alex Lenail. Used to generate diagrams for this notebook. \n",
    "- Alammar, Jay (2016). The Illustrated Transformer [A Visual and Interactive Guide to the Basics of Neural Networks](https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/).\n",
    "- [SINGLE LAYER NEURAL NETWORK - PERCEPTRON MODEL ON THE IRIS DATASET USING HEAVISIDE STEP ACTIVATION FUNCTION](https://www.bogotobogo.com/python/scikit-learn/Perceptron_Model_with_Iris_DataSet.php) by K Hong. For Perceptron Demo.\n",
    "\n",
    "## External Review Material\n",
    "(i.e. Stuff we recommend watching to go to the next level of understanding)\n",
    "\n",
    "- [3 Blue 1 Brown Neural Network Videos](https://youtu.be/aircAruvnKk)\n",
    "- [Andrew Ng Neural Network Introduction Videos](https://www.youtube.com/watch?v=1ZhtwInuOD0&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=43)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_431_Intro_to_NN_Lecture.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S2-NNF (Python3)",
   "language": "python",
   "name": "u4-s2-nnf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
